{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "529c9f8759bede575fc3c1c0a7d57d5ffc5b30205a4d6a007108b8ded7bffe54"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Crear Sesión de Spark\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "source": [
    "## Cargar y mostrar la data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = 'data/UCI_Credit_Card.csv'\n",
    "\n",
    "data_file = spark.read.csv(file_csv, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|default.payment.next.month|\n+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|                         1|\n|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|                         1|\n|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|                         0|\n|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|                         0|\n|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|                         0|\n|  6|    50000|  1|        1|       2| 37|    0|    0|    0|    0|    0|    0|    64400|    57069|    57608|    19394|    19619|    20024|    2500|    1815|     657|    1000|    1000|     800|                         0|\n|  7|    5e+05|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0|   367965|   412023|   445007|   542653|   483003|   473944|   55000|   40000|   38000|   20239|   13750|   13770|                         0|\n|  8|    1e+05|  2|        2|       2| 23|    0|   -1|   -1|    0|    0|   -1|    11876|      380|      601|      221|     -159|      567|     380|     601|       0|     581|    1687|    1542|                         0|\n|  9|   140000|  2|        3|       1| 28|    0|    0|    2|    0|    0|    0|    11285|    14096|    12108|    12211|    11793|     3719|    3329|       0|     432|    1000|    1000|    1000|                         0|\n| 10|    20000|  1|        3|       2| 35|   -2|   -2|   -2|   -2|   -1|   -1|        0|        0|        0|        0|    13007|    13912|       0|       0|       0|   13007|    1122|       0|                         0|\n| 11|    2e+05|  2|        3|       2| 34|    0|    0|    2|    0|    0|   -1|    11073|     9787|     5535|     2513|     1828|     3731|    2306|      12|      50|     300|    3738|      66|                         0|\n| 12|   260000|  2|        1|       2| 51|   -1|   -1|   -1|   -1|   -1|    2|    12261|    21670|     9966|     8517|    22287|    13668|   21818|    9966|    8583|   22301|       0|    3640|                         0|\n| 13|   630000|  2|        2|       2| 41|   -1|    0|   -1|   -1|   -1|   -1|    12137|     6500|     6500|     6500|     6500|     2870|    1000|    6500|    6500|    6500|    2870|       0|                         0|\n| 14|    70000|  1|        2|       2| 30|    1|    2|    2|    0|    0|    2|    65802|    67369|    65701|    66782|    36137|    36894|    3200|       0|    3000|    3000|    1500|       0|                         1|\n| 15|   250000|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0|    70887|    67060|    63561|    59696|    56875|    55512|    3000|    3000|    3000|    3000|    3000|    3000|                         0|\n| 16|    50000|  2|        3|       3| 23|    1|    2|    0|    0|    0|    0|    50614|    29173|    28116|    28771|    29531|    30211|       0|    1500|    1100|    1200|    1300|    1100|                         0|\n| 17|    20000|  1|        1|       2| 24|    0|    0|    2|    2|    2|    2|    15376|    18010|    17428|    18338|    17905|    19104|    3200|       0|    1500|       0|    1650|       0|                         1|\n| 18|   320000|  1|        1|       1| 49|    0|    0|    0|   -1|   -1|   -1|   253286|   246536|   194663|    70074|     5856|   195599|   10358|   10000|   75940|   20000|  195599|   50000|                         0|\n| 19|   360000|  2|        1|       1| 49|    1|   -2|   -2|   -2|   -2|   -2|        0|        0|        0|        0|        0|        0|       0|       0|       0|       0|       0|       0|                         0|\n| 20|   180000|  2|        1|       2| 29|    1|   -2|   -2|   -2|   -2|   -2|        0|        0|        0|        0|        0|        0|       0|       0|       0|       0|       0|       0|                         0|\n+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_file.show()"
   ]
  },
  {
   "source": [
    "### Se requiere fundamentación básica de SQL para las siguientes operaciones (el manejo de los dataframes es análogo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Agregar columna al dataframe\n",
    "\n",
    "- data_file = data_file.withColumn(\"colname\", new_column)\n",
    "\n",
    "#### Filtrar datos con una string SQL\n",
    "\n",
    "- filtered_data = data_file.filter(\"colname > value\")\n",
    "\n",
    "#### Filtrar datos con una columna booleana\n",
    "\n",
    "- filtered_data2 = data_file.filter(data.colname > value)\n",
    "\n",
    "#### Seleccionar el primer conjunto de columnas\n",
    "\n",
    "- selected1 = data_file.select(\"colname1\", \"colname2\", \"colname3\")\n",
    "\n",
    "#### Agrupar por columna \n",
    "\n",
    "- grouped_data = data_file.groupBy(\"colname\")\n",
    "\n",
    "#### Unir dataframes\n",
    "\n",
    "- joined_data = data1.join(data2, on=\"common_colname\", how=\"join_type\")\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Machine Learning Pipeline\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "source": [
    "### Cast columnas a enteros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------+\n|default.payment.next.month|\n+--------------------------+\n|                         1|\n|                         1|\n|                         0|\n|                         0|\n|                         0|\n+--------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# cuando tenga nombres de columnas con puntos usar backticks: df.select(\"`col0.1`\")\n",
    "# data_file = data_file.select('`default.payment.next.month`').show(5)\n",
    "#data_file.withColumnRenamed('`default.payment.next.month`', 'label')"
   ]
  },
  {
   "source": [
    "### Es mejor eliminar los puntos, esto solo es util cuando se trabajan con nested schemas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = data_file.toDF(*(c.replace('.', '_') for c in data_file.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.withColumn(\"LIMIT_BAL\", clean_df.LIMIT_BAL.cast(\"Integer\"))\n",
    "clean_df = clean_df.withColumn(\"SEX\", clean_df.SEX.cast(\"Integer\"))\n",
    "clean_df = clean_df.withColumn(\"EDUCATION\", clean_df.EDUCATION.cast(\"Integer\"))\n",
    "clean_df = clean_df.withColumn(\"MARRIAGE\", clean_df.MARRIAGE.cast(\"Integer\"))"
   ]
  },
  {
   "source": [
    "### Crear labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.withColumn(\"label\", clean_df.default_payment_next_month.cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+\n|label|\n+-----+\n|    1|\n|    1|\n|    0|\n|    0|\n|    0|\n+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "clean_df.select('label').show(5)"
   ]
  },
  {
   "source": [
    "### Eliminar valores faltantes (Nan)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.filter(\"LIMIT_BAL is not NULL and SEX is not NULL and EDUCATION is not NULL and MARRIAGE is not NULL\")"
   ]
  },
  {
   "source": [
    "### Hacer un VectorAssembler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=[\"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\"],\n",
    "                                outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[vec_assembler])"
   ]
  },
  {
   "source": [
    "### Crear un StringIndexer\n",
    "- string_indexer = StringIndexer(inputCol=”inputCol”, outputCol=”outputCol”)\n",
    "### Crear un OneHotEncoder\n",
    "- one_encoder = OneHotEncoder(inputCol=”inputCol”, outputCol=”outputCol”)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Particionar la data en train y test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_data = pipeline.fit(clean_df).transform(clean_df)\n",
    "\n",
    "train, test = pipe_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "source": [
    "## Finalmente ajustar el modelo y ponerlo a prueba\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "source": [
    "### Crear el estimador"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()\n",
    "\n",
    "best_log_reg = logistic_reg.fit(train)\n",
    "\n",
    "predictions = best_log_reg.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}